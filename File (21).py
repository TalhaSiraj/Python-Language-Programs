{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************\n",
      "********************************************************\n",
      "\t\t    Policy Iteration\n",
      "********************************************************\n",
      "********************************************************\n",
      "The Given MDP,\n",
      "Discount factor :  0.8\n",
      "State :  Top Action :  Not to Accelerate\n",
      "State :  Bottom Action :  Not to Accelerate\n",
      "State :  Slide Action :  Not to Accelerate\n",
      "\n",
      "Optimal Policy:\n",
      "State :  Top Action :  Not to Accelerate\n",
      "State :  Bottom Action :  Accelerate\n",
      "State :  Slide Action :  Not to Accelerate\n",
      "\n",
      "Optimal Value:\n",
      "State :  Top Value :  13.8992 at time stamp :  64\n",
      "State :  Bottom Value :  10.73275 at time stamp :  64\n",
      "State :  Slide Value :  10.0862 at time stamp :  64\n",
      "********************************************************\n",
      "Discount Factor Change:-\n",
      "Discount factor :  0.5\n",
      "State :  Top Action :  Not to Accelerate\n",
      "State :  Bottom Action :  Not to Accelerate\n",
      "State :  Slide Action :  Not to Accelerate\n",
      "\n",
      "Optimal Policy:\n",
      "State :  Top Action :  Not to Accelerate\n",
      "State :  Bottom Action :  Accelerate\n",
      "State :  Slide Action :  Not to Accelerate\n",
      "\n",
      "Optimal Value:\n",
      "State :  Top Value :  6.60092 at time stamp :  64\n",
      "State :  Bottom Value :  3.20642 at time stamp :  64\n",
      "State :  Slide Value :  3.10321 at time stamp :  64\n",
      "********************************************************\n",
      "Transition Probability Change:-\n",
      "Discount factor :  0.8\n",
      "State :  Top Action :  Not to Accelerate\n",
      "State :  Bottom Action :  Not to Accelerate\n",
      "State :  Slide Action :  Not to Accelerate\n",
      "\n",
      "Optimal Policy:\n",
      "State :  Top Action :  Not to Accelerate\n",
      "State :  Bottom Action :  Accelerate\n",
      "State :  Slide Action :  Not to Accelerate\n",
      "\n",
      "Optimal Value:\n",
      "State :  Top Value :  13.8992 at time stamp :  64\n",
      "State :  Bottom Value :  10.73275 at time stamp :  64\n",
      "State :  Slide Value :  10.0862 at time stamp :  64\n",
      "********************************************************\n",
      "Reward Value Change:-\n",
      "Discount factor :  0.8\n",
      "State :  Top Action :  Not to Accelerate\n",
      "State :  Bottom Action :  Not to Accelerate\n",
      "State :  Slide Action :  Not to Accelerate\n",
      "\n",
      "Optimal Policy:\n",
      "State :  Top Action :  Not to Accelerate\n",
      "State :  Bottom Action :  Accelerate\n",
      "State :  Slide Action :  Not to Accelerate\n",
      "\n",
      "Optimal Value:\n",
      "State :  Top Value :  13.8992 at time stamp :  64\n",
      "State :  Bottom Value :  10.73275 at time stamp :  64\n",
      "State :  Slide Value :  10.0862 at time stamp :  64\n",
      "\n",
      "********************************************************\n",
      "********************************************************\n",
      "\t\t    value Iteration\t\t\n",
      "********************************************************\n",
      "********************************************************\n",
      "The Given MDP:-\n",
      "\n",
      "Optimal Policy:\n",
      "State :  Top Action :  Not to accelerate\n",
      "State :  Bottom Action :  Accelerate\n",
      "State :  Slide Action :  Not to accelerate\n",
      "\n",
      "Optimal Value:\n",
      "State :  Top Value :  26.05843 at time stamp :  79\n",
      "State :  Bottom Value :  22.9842 at time stamp :  79\n",
      "State :  Slide Value :  22.18578 at time stamp :  79\n",
      "********************************************************\n",
      "Discount Factor Change:-\n",
      "\n",
      "Optimal Policy:\n",
      "State :  Top Action :  Not to accelerate\n",
      "State :  Bottom Action :  Accelerate\n",
      "State :  Slide Action :  Not to accelerate\n",
      "\n",
      "Optimal Value:\n",
      "State :  Top Value :  26.05843 at time stamp :  79\n",
      "State :  Bottom Value :  22.9842 at time stamp :  79\n",
      "State :  Slide Value :  22.18578 at time stamp :  79\n",
      "********************************************************\n",
      "Transition Probability Change:-\n",
      "\n",
      "Optimal Policy:\n",
      "State :  Top Action :  Not to accelerate\n",
      "State :  Bottom Action :  Accelerate\n",
      "State :  Slide Action :  Not to accelerate\n",
      "\n",
      "Optimal Value:\n",
      "State :  Top Value :  26.05843 at time stamp :  79\n",
      "State :  Bottom Value :  22.9842 at time stamp :  79\n",
      "State :  Slide Value :  22.18578 at time stamp :  79\n",
      "********************************************************\n",
      "Reward Value Change:-\n",
      "\n",
      "Optimal Policy:\n",
      "State :  Top Action :  Not to accelerate\n",
      "State :  Bottom Action :  Accelerate\n",
      "State :  Slide Action :  Not to accelerate\n",
      "\n",
      "Optimal Value:\n",
      "State :  Top Value :  26.05843 at time stamp :  79\n",
      "State :  Bottom Value :  22.9842 at time stamp :  79\n",
      "State :  Slide Value :  22.18578 at time stamp :  79\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class Policy_iteration:\n",
    "    def __init__(self):\n",
    "        #temp save action for later use.\n",
    "        self.temp_action=defaultdict(lambda :defaultdict(list))\n",
    "        self.Policy_estariq = defaultdict(list)\n",
    "        self.value_estariq = defaultdict(list)\n",
    "        self.state_prob = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n",
    "        self.state_act_next=defaultdict(lambda: defaultdict(list))\n",
    "        self.state_value_temp = defaultdict(list)\n",
    "        self.Policy = defaultdict(list)\n",
    "        self.state = {\"Top\": 0.0, \"Bottom\": 0.0, \"Slide\": 0.0}\n",
    "        self.state_name = [\"Top\", \"Bottom\", \"Slide\"]\n",
    "        self.Action=[\"a\",\"D_a\"]\n",
    "        self.Discount_factor=[0.8,0.9] #or utility\n",
    "        self.Living_reward=[4.0,1.5]\n",
    "        self.time_stamp=0\n",
    "        self.change_Discount_factor=False\n",
    "        self.change_Transitiion = False\n",
    "        self.change_Reward = False        \n",
    "    def state_probability(self,state,action,next_state,p):\n",
    "        self.state_prob[state][action][next_state].append(p)\n",
    "    def T(self,state,action,next_state):\n",
    "        prob=self.state_prob[state][action].get(next_state)\n",
    "        if(len(prob)>0):\n",
    "            return prob.pop()\n",
    "    def R(self,state,action):\n",
    "        if state == \"Top\":\n",
    "            if action == \"a\":\n",
    "                if self.change_Reward:\n",
    "                    return float(2.5 - 1)\n",
    "                else:\n",
    "                    return float(self.Living_reward[0]-1)\n",
    "            else:\n",
    "                return float(self.Living_reward[0])\n",
    "        else:\n",
    "            if action == \"a\":\n",
    "                return float(self.Living_reward[1] - 1)\n",
    "            else:\n",
    "                return float(self.Living_reward[1])\n",
    "    def Q(self,state,action,next_state,transition):\n",
    "        return transition * (self.R(state, action) + (self.Discount_factor[0] * self.state[next_state]))\n",
    "    def state_probability(self,state,action,next_state,p):\n",
    "        self.state_prob[state][action][next_state].append(p)\n",
    "    def optimal_policy(self):\n",
    "        self.Q_state=[]\n",
    "        self.temp_action_three={\"a\":0,\"D_a\":0}\n",
    "        self.state_value_temp.clear()\n",
    "        k=0\n",
    "        while k<3:\n",
    "            state=self.state_name[k]\n",
    "            i = 0\n",
    "            while i < 2:\n",
    "                action = self.Action[i]\n",
    "                j = 0\n",
    "                while j < 3:\n",
    "                    for next_state in self.state.keys():\n",
    "                        self.Q_state.append(self.provide_child(state, action, next_state, True))\n",
    "                    self.state_prob.clear()\n",
    "                    self.Data_entry()\n",
    "                    j = j + 1\n",
    "                    value = sum(self.Q_state)\n",
    "                    self.temp_action_three[action] = value\n",
    "                    self.Q_state.clear()\n",
    "                i = i + 1\n",
    "            if self.temp_action_three[\"a\"]>self.temp_action_three[\"D_a\"]:\n",
    "                self.Policy[state]=\"a\"\n",
    "                self.state_value_temp[state]=round(max(self.temp_action_three[\"a\"],self.temp_action_three[\"D_a\"]),5)\n",
    "            else:\n",
    "                self.Policy[state] = \"D_a\"\n",
    "                self.state_value_temp[state] =round(max(self.temp_action_three[\"a\"],self.temp_action_three[\"D_a\"]),5)\n",
    "            k=k+1\n",
    "    def provide_child(self,state,action,next_state,Q_value):\n",
    "        if Q_value:\n",
    "            Transition = self.T(state, action, next_state)\n",
    "            return self.Q(state, action, next_state, Transition)\n",
    "        else:\n",
    "            Transition=self.T(state,action,next_state)\n",
    "            value = self.Q(state, action,next_state,Transition)\n",
    "            self.state_value_temp[state].append(value)\n",
    "    def Summision_Q(self):\n",
    "        for state in self.state.keys():\n",
    "            state_value=sum(self.state_value_temp[state])\n",
    "            self.state[state]=state_value\n",
    "    def Show_Initial_policy(self):\n",
    "        print(\"Discount factor : \",self.Discount_factor[0])\n",
    "        for Sta, action in self.Policy.items():\n",
    "            if action == \"a\":\n",
    "                action = \"Accelerate\"\n",
    "            else:\n",
    "                action = \"Not to Accelerate\"\n",
    "            print(\"State : \", Sta, \"Action : \", action)\n",
    "    def initial_policy(self):\n",
    "        for state in self.state.keys():\n",
    "            self.Policy[state] = \"D_a\"\n",
    "    def policy_evalution(self,Evalutaion):\n",
    "        if Evalutaion:\n",
    "            count=0\n",
    "            for state, action in self.Policy.items():\n",
    "                if action == \"a\":\n",
    "                    action=\"Accelerate\"\n",
    "                else:\n",
    "                    action = \"Not to Accelerate\"\n",
    "                self.Policy_estariq[state]=action\n",
    "            for state, action in self.state.items():\n",
    "                self.value_estariq[state]=action\n",
    "            self.optimal_policy()\n",
    "            for state,value in self.state_value_temp.items():\n",
    "                if self.value_estariq[state] == value:\n",
    "                    count=count+1\n",
    "                else:\n",
    "                    self.state[state]=value\n",
    "            self.state_value_temp.clear()\n",
    "            if count == 3:\n",
    "                self.convergence = False\n",
    "                return self.convergence\n",
    "            else:\n",
    "                self.convergence = True\n",
    "                return self.convergence\n",
    "        else:\n",
    "            for state,action in self.Policy.items():\n",
    "                for next_state in self.Policy.keys():\n",
    "                    self.provide_child(state,action,next_state,False)\n",
    "                    self.state_act_next[state][next_state]=action\n",
    "            self.Summision_Q()\n",
    "            return\n",
    "    def Setup(self,skip_state):\n",
    "            j=0\n",
    "            while j<3:\n",
    "                i = 0\n",
    "                State = self.state_name[j]\n",
    "                while i < 3:\n",
    "                    next_state = self.state_name[i]\n",
    "                    if skip_state.__contains__(next_state):\n",
    "                        self.temp_action[State][next_state] = \"a\"\n",
    "                    else:\n",
    "                        self.temp_action[State][next_state] = \"D_a\"\n",
    "                    i = i + 1\n",
    "                j=j+1\n",
    "            return skip_state\n",
    "    def show_optimal_policy_and_value(self):\n",
    "        print(\"\\nOptimal Policy:\")\n",
    "        for Sta, act in self.Policy_estariq.items():\n",
    "            print(\"State : \", Sta, \"Action : \", act)\n",
    "        print(\"\\nOptimal Value:\")\n",
    "        for sta1, value in self.value_estariq.items():\n",
    "            print(\"State : \", sta1, \"Value : \", value, \"at time stamp : \", ob.time_stamp)\n",
    "    def policy_improvement(self):\n",
    "        self.convergence=True\n",
    "        while self.convergence:\n",
    "            self.Data_entry()\n",
    "            self.state_prob.clear()\n",
    "            self.Data_entry()\n",
    "            self.time_stamp = self.time_stamp + 1\n",
    "            self.convergence=self.policy_evalution(True)\n",
    "            self.temp_action.clear()\n",
    "            self.state_prob.clear()\n",
    "            if self.convergence == False:\n",
    "                self.show_optimal_policy_and_value()\n",
    "    def change_of_transition_state(self):\n",
    "        state=\"Bottom\"\n",
    "        action=\"a\"\n",
    "        probability=0.7\n",
    "        for next_state in self.state.keys():\n",
    "            if next_state == \"Top\":\n",
    "                self.state_prob[state][action][next_state].append(probability)\n",
    "            if next_state == \"Bottom\":\n",
    "                self.state_prob[state][action][next_state].append(1-probability)\n",
    "            else:\n",
    "                self.state_prob[state][action][next_state].append(0)\n",
    "    def start(self):\n",
    "        if self.change_Discount_factor:\n",
    "            self.Discount_factor.clear()\n",
    "            self.Discount_factor.append(0.5)\n",
    "        if self.change_Transitiion:\n",
    "            self.change_of_transition_state()\n",
    "        self.Data_entry()\n",
    "        self.initial_policy()\n",
    "        self.Show_Initial_policy()\n",
    "        self.policy_evalution(False)\n",
    "        self.state_prob.clear()\n",
    "        self.policy_improvement()\n",
    "    def Data_entry(self):\n",
    "        self.state_probability(\"Top\", \"a\", \"Top\", 0.8)\n",
    "        self.state_probability(\"Top\", \"a\", \"Slide\", 0.2)\n",
    "        self.state_probability(\"Top\", \"a\", \"Bottom\", 0.0)\n",
    "        self.state_probability(\"Top\", \"D_a\", \"Top\", 0.6)\n",
    "        self.state_probability(\"Top\", \"D_a\", \"Slide\", 0.4)\n",
    "        self.state_probability(\"Top\", \"D_a\", \"Bottom\", 0.0)\n",
    "        self.state_probability(\"Bottom\", \"a\", \"Top\", 0.65)\n",
    "        self.state_probability(\"Bottom\", \"a\", \"Bottom\", 0.35)\n",
    "        self.state_probability(\"Bottom\", \"a\", \"Slide\", 0.0)\n",
    "        self.state_probability(\"Bottom\", \"D_a\", \"Slide\", 0.0)\n",
    "        self.state_probability(\"Bottom\", \"D_a\", \"Top\", 0.0)\n",
    "        self.state_probability(\"Bottom\", \"D_a\", \"Bottom\", 1.0)\n",
    "        self.state_probability(\"Slide\", \"a\", \"Top\", 0.25)\n",
    "        self.state_probability(\"Slide\", \"a\", \"Slide\", 0.65)\n",
    "        self.state_probability(\"Slide\", \"a\", \"Bottom\", 0.1)\n",
    "        self.state_probability(\"Slide\", \"D_a\", \"Bottom\", 1.0)\n",
    "        self.state_probability(\"Slide\", \"D_a\", \"Top\", 0.0)\n",
    "        self.state_probability(\"Slide\", \"D_a\", \"Slide\", 0.0)\n",
    "ob  = Policy_iteration()\n",
    "ob1 = Policy_iteration()\n",
    "ob2 = Policy_iteration()\n",
    "ob3 = Policy_iteration()\n",
    "i=0\n",
    "while i <4:\n",
    "    if i == 0:\n",
    "        print(\"********************************************************\")\n",
    "        print(\"********************************************************\")\n",
    "        print(\"\\t\\t    Policy Iteration\")\n",
    "        print(\"********************************************************\")\n",
    "        print(\"********************************************************\")\n",
    "        print(\"The Given MDP,\")\n",
    "        ob.start()\n",
    "    else:\n",
    "        if i== 1:\n",
    "            print(\"********************************************************\")\n",
    "            print(\"Discount Factor Change:-\")\n",
    "            ob1.change_Discount_factor=True\n",
    "            ob1.start()\n",
    "            ob1.change_Discount_factor = False\n",
    "        else:\n",
    "            if i == 2:\n",
    "                print(\"********************************************************\")\n",
    "                print(\"Transition Probability Change:-\")\n",
    "                ob2.change_Transitiion=True\n",
    "                ob2.start()\n",
    "                ob2.change_Transitiion = False\n",
    "            else:\n",
    "                if i == 3:\n",
    "                    print(\"********************************************************\")\n",
    "                    print(\"Reward Value Change:-\")\n",
    "                    ob3.change_Reward= True\n",
    "                    ob3.start()\n",
    "                    ob3.change_Reward = True\n",
    "\n",
    "    i=i+1\n",
    "    \n",
    "class value_iteration:\n",
    "    def __init__(self):\n",
    "        self.state_value_temp = defaultdict(list)\n",
    "        self.state=defaultdict(list)\n",
    "        self.Policy = defaultdict(list)\n",
    "        self.state_prob=defaultdict(lambda:defaultdict(lambda :defaultdict(list)))\n",
    "        self.state={ \"Top\":0.0,\"Bottom\":0.0,\"Slide\":0.0 }\n",
    "        self.Action=[\"a\",\"D_a\"]\n",
    "        self.Discount_factor=[0.9,0.8]\n",
    "        self.Living_reward=[4.0,1.5]\n",
    "        self.time_stamp=0\n",
    "        self.change_Discount_factor = False\n",
    "        self.change_Transitiion = False\n",
    "        self.change_Reward = False\n",
    "    def T(self,state,action,next_state):\n",
    "        prob=self.state_prob[state][action].get(next_state)\n",
    "        if(len(prob)>0):\n",
    "            return prob.pop()\n",
    "    def R(self,state,action):\n",
    "        if state == \"Top\":\n",
    "            if action == \"a\":\n",
    "                if self.change_Reward:\n",
    "                    return float(2.5 - 1)\n",
    "                else:\n",
    "                    return float(self.Living_reward[0] - 1)\n",
    "            else:\n",
    "                return float(self.Living_reward[0])\n",
    "        else:\n",
    "            if  action == \"a\" :\n",
    "                return float(self.Living_reward[1] - 1)\n",
    "            else:\n",
    "                return float(self.Living_reward[1])\n",
    "    def Q(self,state,action,next_state,transition):\n",
    "        return transition*(self.R(state,action)+(self.Discount_factor[0]*self.state[next_state]))\n",
    "    def state_probability(self,state,action,next_state,p):\n",
    "        self.state_prob[state][action][next_state].append(p)\n",
    "    def provide_child(self,state,action,next_state):\n",
    "        Transition=self.T(state,action,next_state)\n",
    "        value = self.Q(state, action,next_state,Transition)\n",
    "        self.state_value_temp[value].append(next_state)\n",
    "    def setup(self):\n",
    "        Q1=0\n",
    "        Q2=0\n",
    "        action=0\n",
    "        i=0\n",
    "        for State in self.state.keys():\n",
    "            while i<2 :\n",
    "                for next_state in self.state.keys():\n",
    "                    self.provide_child(State, self.Action[action], next_state)\n",
    "                if action == 0 :\n",
    "                    Q1=sum(self.state_value_temp.keys())\n",
    "                    self.state_value_temp.clear()\n",
    "                else:\n",
    "                    Q2 = sum(self.state_value_temp.keys())\n",
    "                    self.state_value_temp.clear()\n",
    "                action = 1\n",
    "                i=i+1\n",
    "            i=0\n",
    "            action=0\n",
    "            Max=max(Q1,Q2)\n",
    "            Max=round(Max,5)\n",
    "            value=self.state[State]\n",
    "            value=round(value,5)\n",
    "            if value<Max:\n",
    "                self.state[State]=Max\n",
    "                self.Policy[State]=self.Return_action(Q1,Q2)\n",
    "            if value == Max:\n",
    "                self.convergence=False\n",
    "        self.time_stamp =self.time_stamp+1\n",
    "    def Return_action(self,Q1,Q2):\n",
    "                if Q1>Q2:\n",
    "                    return \"Accelerate\"\n",
    "                else:\n",
    "                    return \"Not to accelerate\"\n",
    "    def Data_entry(self):\n",
    "        self.state_probability(\"Top\", \"a\", \"Top\", 0.8)\n",
    "        self.state_probability(\"Top\", \"a\", \"Slide\", 0.2)\n",
    "        self.state_probability(\"Top\", \"a\", \"Bottom\", 0.0)\n",
    "        self.state_probability(\"Top\", \"D_a\", \"Top\", 0.6)\n",
    "        self.state_probability(\"Top\", \"D_a\", \"Slide\", 0.4)\n",
    "        self.state_probability(\"Top\", \"D_a\", \"Bottom\", 0.0)\n",
    "        self.state_probability(\"Bottom\", \"a\", \"Top\", 0.65)\n",
    "        self.state_probability(\"Bottom\", \"a\", \"Bottom\", 0.35)\n",
    "        self.state_probability(\"Bottom\", \"a\", \"Slide\", 0.0)\n",
    "        self.state_probability(\"Bottom\", \"D_a\", \"Slide\", 0.0)\n",
    "        self.state_probability(\"Bottom\", \"D_a\", \"Top\", 0.0)\n",
    "        self.state_probability(\"Bottom\", \"D_a\", \"Bottom\", 1.0)\n",
    "        self.state_probability(\"Slide\", \"a\", \"Top\", 0.25)\n",
    "        self.state_probability(\"Slide\", \"a\", \"Slide\", 0.65)\n",
    "        self.state_probability(\"Slide\", \"a\", \"Bottom\", 0.1)\n",
    "        self.state_probability(\"Slide\", \"D_a\", \"Bottom\", 1.0)\n",
    "        self.state_probability(\"Slide\", \"D_a\", \"Top\", 0.0)\n",
    "        self.state_probability(\"Slide\", \"D_a\", \"Slide\", 0.0)\n",
    "    def show_optimal_policy_and_value(self):\n",
    "        print(\"\\nOptimal Policy:\")\n",
    "        for Sta, act in ob.Policy.items():\n",
    "            print(\"State : \", Sta, \"Action : \", act)\n",
    "        print(\"\\nOptimal Value:\")\n",
    "        for sta1, value in ob.state.items():\n",
    "            print(\"State : \", sta1, \"Value : \", value, \"at time stamp : \", ob.time_stamp)\n",
    "    def change_of_transition_state(self):\n",
    "        state=\"Bottom\"\n",
    "        action=\"a\"\n",
    "        probability=0.7\n",
    "        for next_state in self.state.keys():\n",
    "            if next_state == \"Top\":\n",
    "                self.state_prob[state][action][next_state].append(probability)\n",
    "            if next_state == \"Bottom\":\n",
    "                self.state_prob[state][action][next_state].append(1-probability)\n",
    "            else:\n",
    "                self.state_prob[state][action][next_state].append(0)\n",
    "    def start(self):\n",
    "        if self.change_Discount_factor:\n",
    "            self.Discount_factor.clear()\n",
    "            self.Discount_factor.append(0.5)\n",
    "        self.convergence=True\n",
    "        while self.convergence:\n",
    "            self.Data_entry()\n",
    "            if self.change_Transitiion:\n",
    "                self.change_of_transition_state()\n",
    "            self.setup()\n",
    "            self.state_prob.clear()\n",
    "            if self.convergence==False:\n",
    "                 self.show_optimal_policy_and_value()                    \n",
    "ob = value_iteration()\n",
    "Ob = value_iteration()\n",
    "oB = value_iteration()\n",
    "OB = value_iteration()\n",
    "i=0\n",
    "while i <4:\n",
    "    if i == 0:\n",
    "        print(\"\\n********************************************************\")\n",
    "        print(\"********************************************************\")\n",
    "        print(\"\\t\\t    value Iteration\\t\\t\")\n",
    "        print(\"********************************************************\")\n",
    "        print(\"********************************************************\")\n",
    "        print(\"The Given MDP:-\")\n",
    "        ob.start()\n",
    "    else:\n",
    "        if i== 1:\n",
    "            print(\"********************************************************\")\n",
    "            print(\"Discount Factor Change:-\")\n",
    "            Ob.change_Discount_factor=True\n",
    "            Ob.start()\n",
    "            Ob.change_Discount_factor = False\n",
    "        else:\n",
    "            if i == 2:\n",
    "                print(\"********************************************************\")\n",
    "                print(\"Transition Probability Change:-\")\n",
    "                oB.change_Transitiion=True\n",
    "                oB.start()\n",
    "                oB.change_Transitiion = False\n",
    "            else:\n",
    "                if i == 3:\n",
    "                    print(\"********************************************************\")\n",
    "                    print(\"Reward Value Change:-\")\n",
    "                    OB.change_Reward= True\n",
    "                    OB.start()\n",
    "                    OB.change_Reward = True\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
